{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbef6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.datasets.factory import make_dataset\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from lerobot.configs.default import DatasetConfig\n",
    "from lerobot.datasets.utils import dataset_to_policy_features\n",
    "from lerobot.optim.factory import make_optimizer_and_scheduler\n",
    "from lerobot.datasets.utils import cycle\n",
    "from lerobot.configs.types import FeatureType\n",
    "from lerobot.utils.utils import (\n",
    "    format_big_number,\n",
    "    get_safe_torch_device,\n",
    "    has_method,\n",
    "    init_logging,\n",
    ")\n",
    "import torch\n",
    "from src.policies.baseline.configuration import BaselineConfig\n",
    "from src.policies.baseline.processor import make_baseline_pre_post_processors\n",
    "from src.policies.baseline.modeling import BaselinePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9d3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./dataset/transformed_data_obj_pose\"\n",
    "CKPT_PATH = \"./ckpt/baseline_model\"\n",
    "LOG_EVERY = 100\n",
    "TOTAL_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92edef6",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "This is the baseline configurations that can be edited. \n",
    "\n",
    "```\n",
    "@PreTrainedConfig.register_subclass(\"omy_baseline\")\n",
    "@dataclass\n",
    "class BaselineConfig(PreTrainedConfig):\n",
    "    # Input / output structure.\n",
    "    n_obs_steps: int = 1\n",
    "    chunk_size: int = 5\n",
    "    n_action_steps: int = 5\n",
    "\n",
    "    # Architecture.\n",
    "    backbone: str = 'mlp' # 'mlp' or 'transformer'\n",
    "    # Vision encoder\n",
    "    vision_backbone: str =\"facebook/dinov3-vitb16-pretrain-lvd1689m\" #\"facebook/dinov2-base\"\n",
    "    projection_dim : int = 128\n",
    "    freeze_backbone:  bool = True\n",
    "\n",
    "\n",
    "    # Num hidden layers\n",
    "    n_hidden_layers: int = 5\n",
    "    hidden_dim: int = 512   \n",
    "\n",
    "    ## For transformer-based architectures\n",
    "    n_heads: int = 4\n",
    "    dim_feedforward: int = 2048\n",
    "    feedforward_activation: str = \"gelu\"\n",
    "    dropout: float = 0.1\n",
    "    pre_norm: bool = True\n",
    "    n_encoder_layers: int = 6\n",
    "\n",
    "    # Training preset\n",
    "    optimizer_lr: float = 1e-3\n",
    "    optimizer_weight_decay: float = 1e-6\n",
    "\n",
    "    # Learning rate scheduler parameters \n",
    "    lr_warmup_steps: int = 1000\n",
    "    total_training_steps: int = 500000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118d21f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lerobot.configs.policies:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load Dataset and Configurations     \n",
    "'''\n",
    "# Dataset Config\n",
    "dataset_cfg = DatasetConfig(\"transformed_data\")\n",
    "dataset_cfg.root = ROOT\n",
    "pipeline_cfg = TrainPipelineConfig(dataset_cfg)\n",
    "\n",
    "# Policy Config\n",
    "cfg = BaselineConfig(\n",
    "    chunk_size=10,\n",
    "    n_action_steps=10,\n",
    "    backbone='mlp',\n",
    "    optimizer_lr= 5e-4,\n",
    "    n_hidden_layers=10,\n",
    "    hidden_dim=512,\n",
    "    # If you are using image features, uncomment the following line\n",
    "    vision_backbone='facebook/dinov3-vitb16-pretrain-lvd1689m',#\"facebook/dinov2-base\", **You need access to use this model** Use dinov2 if you don't have access\n",
    "    projection_dim=128,\n",
    "    freeze_backbone=True,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798de6e9",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15fbabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kwargs and configure pipeline\n",
    "kwargs = {}\n",
    "pipeline_cfg.policy = cfg\n",
    "pipeline_cfg.optimizer = cfg.get_optimizer_preset()\n",
    "pipeline_cfg.scheduler = cfg.get_scheduler_preset()\n",
    "\n",
    "# Create Dataset\n",
    "# Meta data is for loading dataset statistics and feature information\n",
    "dataset = make_dataset(pipeline_cfg)\n",
    "ds_meta = dataset.meta\n",
    "features = dataset_to_policy_features(ds_meta.features)\n",
    "cfg.output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
    "cfg.input_features = {key: ft for key, ft in features.items() if key not in cfg.output_features}\n",
    "kwargs[\"config\"] = cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155cf171",
   "metadata": {},
   "source": [
    "## Load Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047f9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor, postprocessor = make_baseline_pre_post_processors(\n",
    "        config=cfg,\n",
    "        dataset_stats= ds_meta.stats\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7fee07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselinePolicy(\n",
       "  (model): BaselineModel(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=34, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (9): ReLU()\n",
       "      (10): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (13): ReLU()\n",
       "      (14): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (15): ReLU()\n",
       "      (16): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (17): ReLU()\n",
       "      (18): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (19): ReLU()\n",
       "      (20): Linear(in_features=512, out_features=70, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''`\n",
    "Instantiate Policy\n",
    "'''\n",
    "policy = BaselinePolicy(**kwargs)\n",
    "policy.to(pipeline_cfg.policy.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6143b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create Optimizer and Scheduler\n",
    "'''\n",
    "optimizer, lr_scheduler = make_optimizer_and_scheduler(pipeline_cfg, policy)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size= 64,\n",
    "    drop_last=False,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f8c0f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2M\n",
      "Number of trainable parameters: 2M\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check Parameter Counts\n",
    "'''\n",
    "trainable_params = [p for p in policy.parameters() if p.requires_grad]\n",
    "total_params = list(policy.parameters())\n",
    "print(f\"Total number of parameters: {format_big_number(sum(p.numel() for p in total_params))}\")\n",
    "print(f\"Number of trainable parameters: {format_big_number(sum(p.numel() for p in trainable_params))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d47b4",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/100\n",
      "Step: 100, Loss: 0.5228, learning rate: 0.000251\n",
      "Starting epoch 2/100\n",
      "Step: 200, Loss: 0.4257, learning rate: 0.000500\n",
      "Step: 300, Loss: 0.3678, learning rate: 0.000500\n",
      "Starting epoch 3/100\n",
      "Step: 400, Loss: 0.3519, learning rate: 0.000500\n",
      "Starting epoch 4/100\n",
      "Step: 500, Loss: 0.3025, learning rate: 0.000500\n",
      "Step: 600, Loss: 0.2971, learning rate: 0.000500\n",
      "Starting epoch 5/100\n",
      "Step: 700, Loss: 0.2737, learning rate: 0.000500\n",
      "Starting epoch 6/100\n",
      "Step: 800, Loss: 0.2150, learning rate: 0.000500\n",
      "Step: 900, Loss: 0.2311, learning rate: 0.000500\n",
      "Starting epoch 7/100\n",
      "Step: 1000, Loss: 0.2180, learning rate: 0.000500\n",
      "Starting epoch 8/100\n",
      "Step: 1100, Loss: 0.1947, learning rate: 0.000500\n",
      "Step: 1200, Loss: 0.1975, learning rate: 0.000500\n",
      "Starting epoch 9/100\n",
      "Step: 1300, Loss: 0.2002, learning rate: 0.000500\n",
      "Starting epoch 10/100\n",
      "Step: 1400, Loss: 0.2222, learning rate: 0.000500\n",
      "Step: 1500, Loss: 0.1707, learning rate: 0.000500\n",
      "Starting epoch 11/100\n",
      "Step: 1600, Loss: 0.1797, learning rate: 0.000500\n",
      "Starting epoch 12/100\n",
      "Step: 1700, Loss: 0.1765, learning rate: 0.000500\n",
      "Step: 1800, Loss: 0.1752, learning rate: 0.000500\n",
      "Starting epoch 13/100\n",
      "Step: 1900, Loss: 0.1831, learning rate: 0.000500\n",
      "Starting epoch 14/100\n",
      "Step: 2000, Loss: 0.1771, learning rate: 0.000500\n",
      "Step: 2100, Loss: 0.1687, learning rate: 0.000499\n",
      "Starting epoch 15/100\n",
      "Step: 2200, Loss: 0.1788, learning rate: 0.000499\n",
      "Starting epoch 16/100\n",
      "Step: 2300, Loss: 0.1794, learning rate: 0.000499\n",
      "Step: 2400, Loss: 0.1865, learning rate: 0.000499\n",
      "Starting epoch 17/100\n",
      "Step: 2500, Loss: 0.1694, learning rate: 0.000499\n",
      "Step: 2600, Loss: 0.1813, learning rate: 0.000499\n",
      "Starting epoch 18/100\n",
      "Step: 2700, Loss: 0.1670, learning rate: 0.000499\n",
      "Starting epoch 19/100\n",
      "Step: 2800, Loss: 0.1623, learning rate: 0.000499\n",
      "Step: 2900, Loss: 0.1686, learning rate: 0.000499\n",
      "Starting epoch 20/100\n",
      "Step: 3000, Loss: 0.1684, learning rate: 0.000499\n",
      "Starting epoch 21/100\n",
      "Step: 3100, Loss: 0.1426, learning rate: 0.000499\n",
      "Step: 3200, Loss: 0.1583, learning rate: 0.000499\n",
      "Starting epoch 22/100\n",
      "Step: 3300, Loss: 0.1341, learning rate: 0.000499\n",
      "Starting epoch 23/100\n",
      "Step: 3400, Loss: 0.1401, learning rate: 0.000499\n",
      "Step: 3500, Loss: 0.1412, learning rate: 0.000499\n",
      "Starting epoch 24/100\n",
      "Step: 3600, Loss: 0.1558, learning rate: 0.000498\n",
      "Starting epoch 25/100\n",
      "Step: 3700, Loss: 0.1457, learning rate: 0.000498\n",
      "Step: 3800, Loss: 0.1418, learning rate: 0.000498\n",
      "Starting epoch 26/100\n",
      "Step: 3900, Loss: 0.1470, learning rate: 0.000498\n",
      "Starting epoch 27/100\n",
      "Step: 4000, Loss: 0.1386, learning rate: 0.000498\n",
      "Step: 4100, Loss: 0.1426, learning rate: 0.000498\n",
      "Starting epoch 28/100\n",
      "Step: 4200, Loss: 0.1335, learning rate: 0.000498\n",
      "Starting epoch 29/100\n",
      "Step: 4300, Loss: 0.1291, learning rate: 0.000498\n",
      "Step: 4400, Loss: 0.1345, learning rate: 0.000498\n",
      "Starting epoch 30/100\n",
      "Step: 4500, Loss: 0.1202, learning rate: 0.000498\n",
      "Starting epoch 31/100\n",
      "Step: 4600, Loss: 0.1264, learning rate: 0.000497\n",
      "Step: 4700, Loss: 0.1176, learning rate: 0.000497\n",
      "Starting epoch 32/100\n",
      "Step: 4800, Loss: 0.1299, learning rate: 0.000497\n",
      "Starting epoch 33/100\n",
      "Step: 4900, Loss: 0.1276, learning rate: 0.000497\n",
      "Step: 5000, Loss: 0.1310, learning rate: 0.000497\n",
      "Starting epoch 34/100\n",
      "Step: 5100, Loss: 0.1199, learning rate: 0.000497\n",
      "Step: 5200, Loss: 0.1206, learning rate: 0.000497\n",
      "Starting epoch 35/100\n",
      "Step: 5300, Loss: 0.1206, learning rate: 0.000497\n",
      "Starting epoch 36/100\n",
      "Step: 5400, Loss: 0.1128, learning rate: 0.000496\n",
      "Step: 5500, Loss: 0.1176, learning rate: 0.000496\n",
      "Starting epoch 37/100\n",
      "Step: 5600, Loss: 0.1197, learning rate: 0.000496\n",
      "Starting epoch 38/100\n",
      "Step: 5700, Loss: 0.1111, learning rate: 0.000496\n",
      "Step: 5800, Loss: 0.1213, learning rate: 0.000496\n",
      "Starting epoch 39/100\n",
      "Step: 5900, Loss: 0.1143, learning rate: 0.000496\n",
      "Starting epoch 40/100\n",
      "Step: 6000, Loss: 0.1092, learning rate: 0.000496\n",
      "Step: 6100, Loss: 0.1107, learning rate: 0.000496\n",
      "Starting epoch 41/100\n",
      "Step: 6200, Loss: 0.0944, learning rate: 0.000495\n",
      "Starting epoch 42/100\n",
      "Step: 6300, Loss: 0.1186, learning rate: 0.000495\n",
      "Step: 6400, Loss: 0.1014, learning rate: 0.000495\n",
      "Starting epoch 43/100\n",
      "Step: 6500, Loss: 0.1058, learning rate: 0.000495\n",
      "Starting epoch 44/100\n",
      "Step: 6600, Loss: 0.1002, learning rate: 0.000495\n",
      "Step: 6700, Loss: 0.0937, learning rate: 0.000495\n",
      "Starting epoch 45/100\n",
      "Step: 6800, Loss: 0.1040, learning rate: 0.000494\n",
      "Starting epoch 46/100\n",
      "Step: 6900, Loss: 0.0977, learning rate: 0.000494\n",
      "Step: 7000, Loss: 0.0979, learning rate: 0.000494\n",
      "Starting epoch 47/100\n",
      "Step: 7100, Loss: 0.1038, learning rate: 0.000494\n",
      "Starting epoch 48/100\n",
      "Step: 7200, Loss: 0.0971, learning rate: 0.000494\n",
      "Step: 7300, Loss: 0.1014, learning rate: 0.000494\n",
      "Starting epoch 49/100\n",
      "Step: 7400, Loss: 0.0954, learning rate: 0.000493\n",
      "Starting epoch 50/100\n",
      "Step: 7500, Loss: 0.0950, learning rate: 0.000493\n",
      "Step: 7600, Loss: 0.0900, learning rate: 0.000493\n",
      "Starting epoch 51/100\n",
      "Step: 7700, Loss: 0.0919, learning rate: 0.000493\n",
      "Step: 7800, Loss: 0.1033, learning rate: 0.000493\n",
      "Starting epoch 52/100\n",
      "Step: 7900, Loss: 0.0960, learning rate: 0.000492\n",
      "Starting epoch 53/100\n",
      "Step: 8000, Loss: 0.0864, learning rate: 0.000492\n",
      "Step: 8100, Loss: 0.0970, learning rate: 0.000492\n",
      "Starting epoch 54/100\n",
      "Step: 8200, Loss: 0.0877, learning rate: 0.000492\n",
      "Starting epoch 55/100\n",
      "Step: 8300, Loss: 0.0857, learning rate: 0.000492\n",
      "Step: 8400, Loss: 0.0859, learning rate: 0.000492\n",
      "Starting epoch 56/100\n",
      "Step: 8500, Loss: 0.0852, learning rate: 0.000491\n",
      "Starting epoch 57/100\n",
      "Step: 8600, Loss: 0.0846, learning rate: 0.000491\n",
      "Step: 8700, Loss: 0.0930, learning rate: 0.000491\n",
      "Starting epoch 58/100\n",
      "Step: 8800, Loss: 0.0837, learning rate: 0.000491\n",
      "Starting epoch 59/100\n",
      "Step: 8900, Loss: 0.0813, learning rate: 0.000490\n",
      "Step: 9000, Loss: 0.0887, learning rate: 0.000490\n",
      "Starting epoch 60/100\n",
      "Step: 9100, Loss: 0.0808, learning rate: 0.000490\n",
      "Starting epoch 61/100\n",
      "Step: 9200, Loss: 0.0808, learning rate: 0.000490\n",
      "Step: 9300, Loss: 0.0925, learning rate: 0.000490\n",
      "Starting epoch 62/100\n",
      "Step: 9400, Loss: 0.0921, learning rate: 0.000489\n",
      "Starting epoch 63/100\n",
      "Step: 9500, Loss: 0.0781, learning rate: 0.000489\n",
      "Step: 9600, Loss: 0.0846, learning rate: 0.000489\n",
      "Starting epoch 64/100\n",
      "Step: 9700, Loss: 0.0915, learning rate: 0.000489\n",
      "Starting epoch 65/100\n",
      "Step: 9800, Loss: 0.0831, learning rate: 0.000488\n",
      "Step: 9900, Loss: 0.0835, learning rate: 0.000488\n",
      "Starting epoch 66/100\n",
      "Step: 10000, Loss: 0.0876, learning rate: 0.000488\n",
      "Starting epoch 67/100\n",
      "Step: 10100, Loss: 0.0785, learning rate: 0.000488\n",
      "Step: 10200, Loss: 0.0803, learning rate: 0.000488\n",
      "Starting epoch 68/100\n",
      "Step: 10300, Loss: 0.0735, learning rate: 0.000487\n",
      "Step: 10400, Loss: 0.0798, learning rate: 0.000487\n",
      "Starting epoch 69/100\n",
      "Step: 10500, Loss: 0.0732, learning rate: 0.000487\n",
      "Starting epoch 70/100\n",
      "Step: 10600, Loss: 0.0722, learning rate: 0.000487\n",
      "Step: 10700, Loss: 0.0735, learning rate: 0.000486\n",
      "Starting epoch 71/100\n",
      "Step: 10800, Loss: 0.0760, learning rate: 0.000486\n",
      "Starting epoch 72/100\n",
      "Step: 10900, Loss: 0.0772, learning rate: 0.000486\n",
      "Step: 11000, Loss: 0.0768, learning rate: 0.000486\n",
      "Starting epoch 73/100\n",
      "Step: 11100, Loss: 0.0696, learning rate: 0.000485\n",
      "Starting epoch 74/100\n",
      "Step: 11200, Loss: 0.0705, learning rate: 0.000485\n",
      "Step: 11300, Loss: 0.0876, learning rate: 0.000485\n",
      "Starting epoch 75/100\n",
      "Step: 11400, Loss: 0.0674, learning rate: 0.000484\n",
      "Starting epoch 76/100\n",
      "Step: 11500, Loss: 0.0681, learning rate: 0.000484\n",
      "Step: 11600, Loss: 0.0675, learning rate: 0.000484\n",
      "Starting epoch 77/100\n",
      "Step: 11700, Loss: 0.0598, learning rate: 0.000484\n",
      "Starting epoch 78/100\n",
      "Step: 11800, Loss: 0.0713, learning rate: 0.000483\n",
      "Step: 11900, Loss: 0.0737, learning rate: 0.000483\n",
      "Starting epoch 79/100\n",
      "Step: 12000, Loss: 0.0646, learning rate: 0.000483\n",
      "Starting epoch 80/100\n",
      "Step: 12100, Loss: 0.0687, learning rate: 0.000483\n",
      "Step: 12200, Loss: 0.0673, learning rate: 0.000482\n",
      "Starting epoch 81/100\n",
      "Step: 12300, Loss: 0.0815, learning rate: 0.000482\n",
      "Starting epoch 82/100\n",
      "Step: 12400, Loss: 0.0640, learning rate: 0.000482\n",
      "Step: 12500, Loss: 0.0652, learning rate: 0.000481\n",
      "Starting epoch 83/100\n",
      "Step: 12600, Loss: 0.0586, learning rate: 0.000481\n",
      "Starting epoch 84/100\n",
      "Step: 12700, Loss: 0.0712, learning rate: 0.000481\n",
      "Step: 12800, Loss: 0.0697, learning rate: 0.000480\n",
      "Starting epoch 85/100\n",
      "Step: 12900, Loss: 0.0705, learning rate: 0.000480\n",
      "Step: 13000, Loss: 0.0641, learning rate: 0.000480\n",
      "Starting epoch 86/100\n",
      "Step: 13100, Loss: 0.0702, learning rate: 0.000480\n",
      "Starting epoch 87/100\n",
      "Step: 13200, Loss: 0.0608, learning rate: 0.000479\n",
      "Step: 13300, Loss: 0.0621, learning rate: 0.000479\n",
      "Starting epoch 88/100\n",
      "Step: 13400, Loss: 0.0626, learning rate: 0.000479\n",
      "Starting epoch 89/100\n",
      "Step: 13500, Loss: 0.0661, learning rate: 0.000478\n",
      "Step: 13600, Loss: 0.0675, learning rate: 0.000478\n",
      "Starting epoch 90/100\n",
      "Step: 13700, Loss: 0.0674, learning rate: 0.000478\n",
      "Starting epoch 91/100\n",
      "Step: 13800, Loss: 0.0619, learning rate: 0.000477\n",
      "Step: 13900, Loss: 0.0629, learning rate: 0.000477\n",
      "Starting epoch 92/100\n",
      "Step: 14000, Loss: 0.0715, learning rate: 0.000477\n",
      "Starting epoch 93/100\n",
      "Step: 14100, Loss: 0.0640, learning rate: 0.000476\n",
      "Step: 14200, Loss: 0.0562, learning rate: 0.000476\n",
      "Starting epoch 94/100\n",
      "Step: 14300, Loss: 0.0584, learning rate: 0.000476\n",
      "Starting epoch 95/100\n",
      "Step: 14400, Loss: 0.0627, learning rate: 0.000475\n",
      "Step: 14500, Loss: 0.0629, learning rate: 0.000475\n",
      "Starting epoch 96/100\n",
      "Step: 14600, Loss: 0.0600, learning rate: 0.000475\n",
      "Starting epoch 97/100\n",
      "Step: 14700, Loss: 0.0500, learning rate: 0.000474\n",
      "Step: 14800, Loss: 0.0653, learning rate: 0.000474\n",
      "Starting epoch 98/100\n",
      "Step: 14900, Loss: 0.0616, learning rate: 0.000474\n",
      "Starting epoch 99/100\n",
      "Step: 15000, Loss: 0.0601, learning rate: 0.000473\n",
      "Step: 15100, Loss: 0.0558, learning rate: 0.000473\n",
      "Starting epoch 100/100\n",
      "Step: 15200, Loss: 0.0568, learning rate: 0.000473\n",
      "Step: 15300, Loss: 0.0628, learning rate: 0.000472\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Loop\n",
    "'''\n",
    "device = get_safe_torch_device(pipeline_cfg.policy.device, log=True)\n",
    "step = 0\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    print(f\"Starting epoch {epoch+1}/{TOTAL_EPOCHS}\")\n",
    "    for batch in dataloader:\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device, non_blocking=True)\n",
    "        policy.train()\n",
    "        batch = preprocessor(batch)\n",
    "        loss, output_dict = policy.forward(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step through pytorch scheduler at every batch instead of epoch\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "        step += 1\n",
    "        if step % LOG_EVERY == 0:\n",
    "            print(f\"Step: {step}, Loss: {loss.item():.4f}, learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "# Save checkpoint at the end of training\n",
    "policy.save_pretrained(CKPT_PATH)\n",
    "preprocessor.save_pretrained(CKPT_PATH)\n",
    "postprocessor.save_pretrained(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7437858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
